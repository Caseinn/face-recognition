{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 1 – Imports & Basic Config\n",
    "# ============================================\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Paths ----\n",
    "MODEL_PATH = \"models/best_facenet_arcface_kfold5.pth\"\n",
    "LABEL_MAP_PATH = \"models/label_map.json\"\n",
    "# Path to your test images directory.\n",
    "# You can place the \"Test\" folder in your project root or adjust the path accordingly.\n",
    "TEST_DIR = \"./Test\"\n",
    "OUTPUT_CSV = \"predictions.csv\"\n",
    "\n",
    "# ---- Image / crop config ----\n",
    "IMG_SIZE = 160\n",
    "TARGET_SIZE = (384, 384)\n",
    "MARGIN_RATIO = 0.15\n",
    "MAX_DIM = 1600\n",
    "MIN_DIM = 256\n",
    "USE_CENTER_FALLBACK = True\n",
    "\n",
    "# ---- Normalize  ----\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78983e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 2 – Load Label Map\n",
    "# ============================================\n",
    "with open(LABEL_MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    label_map = json.load(f)   # {\"NamaOrang\": idx}\n",
    "\n",
    "idx_to_name = {v: k for k, v in label_map.items()}\n",
    "num_classes = len(label_map)\n",
    "\n",
    "print(\"num_classes:\", num_classes)\n",
    "print(\"example mapping:\", list(label_map.items())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 3 – MediaPipe Face Detection + Robust Crop\n",
    "# ============================================\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "\n",
    "def run_face_detection(img_rgb: np.ndarray):\n",
    "    # 1st pass: model_selection=1\n",
    "    with mp_face_detection.FaceDetection(\n",
    "        model_selection=1, min_detection_confidence=0.5\n",
    "    ) as fd:\n",
    "        res = fd.process(img_rgb)\n",
    "        if res.detections:\n",
    "            return res, img_rgb.shape[1], img_rgb.shape[0]\n",
    "\n",
    "    # 2nd pass: fallback\n",
    "    with mp_face_detection.FaceDetection(\n",
    "        model_selection=0, min_detection_confidence=0.3\n",
    "    ) as fd:\n",
    "        res = fd.process(img_rgb)\n",
    "        return res, img_rgb.shape[1], img_rgb.shape[0]\n",
    "\n",
    "\n",
    "def pick_best_detection(detections, w: int, h: int):\n",
    "    best = None\n",
    "    best_score = -1.0\n",
    "    for det in detections:\n",
    "        bbox = det.location_data.relative_bounding_box\n",
    "        score = det.score[0]\n",
    "        area = max(bbox.width * bbox.height, 1e-6)\n",
    "        combined = score * area\n",
    "        if combined > best_score:\n",
    "            best_score = combined\n",
    "            best = det\n",
    "    return best\n",
    "\n",
    "\n",
    "def safe_center_crop(img: np.ndarray) -> Optional[Image.Image]:\n",
    "    \"\"\"Fallback crop yang dijamin tidak 0x0.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    if h == 0 or w == 0:\n",
    "        return None\n",
    "    side = max(1, int(0.8 * min(h, w)))\n",
    "    cx, cy = w // 2, h // 2\n",
    "    x1 = max(0, cx - side // 2)\n",
    "    y1 = max(0, cy - side // 2)\n",
    "    x2 = min(w, x1 + side)\n",
    "    y2 = min(h, y1 + side)\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    crop = img[y1:y2, x1:x2]\n",
    "    if crop.size == 0:\n",
    "        return None\n",
    "    face = Image.fromarray(crop)\n",
    "    return ImageOps.fit(face, TARGET_SIZE, Image.BICUBIC)\n",
    "\n",
    "\n",
    "def detect_and_crop_from_pil(pil_img: Image.Image) -> Optional[Image.Image]:\n",
    "    \"\"\"\n",
    "    Crop wajah utama, aman dari ZeroDivisionError.\n",
    "    \"\"\"\n",
    "    # Fix orientasi, ke RGB\n",
    "    pil_img = ImageOps.exif_transpose(pil_img).convert(\"RGB\")\n",
    "    img = np.array(pil_img)\n",
    "    orig = img.copy()\n",
    "    oh, ow = orig.shape[:2]\n",
    "\n",
    "    # Scale up\n",
    "    short_side = min(oh, ow)\n",
    "    if short_side < MIN_DIM:\n",
    "        scale = MIN_DIM / float(short_side)\n",
    "        new_w = int(ow * scale)\n",
    "        new_h = int(oh * scale)\n",
    "        pil_img = pil_img.resize((new_w, new_h), Image.BICUBIC)\n",
    "        img = np.array(pil_img)\n",
    "\n",
    "    # Scale down\n",
    "    h, w = img.shape[:2]\n",
    "    if max(h, w) > MAX_DIM:\n",
    "        scale = MAX_DIM / float(max(h, w))\n",
    "        new_w = int(w * scale)\n",
    "        new_h = int(h * scale)\n",
    "        pil_img = pil_img.resize((new_w, new_h), Image.BICUBIC)\n",
    "        img = np.array(pil_img)\n",
    "        h, w = new_h, new_w\n",
    "\n",
    "    # Detection @ resized\n",
    "    results, w, h = run_face_detection(img)\n",
    "\n",
    "    # Coba di resolusi awal kalau gagal\n",
    "    if not results or not results.detections:\n",
    "        img = orig\n",
    "        h, w = img.shape[:2]\n",
    "        results, w, h = run_face_detection(img)\n",
    "\n",
    "    if not results or not results.detections:\n",
    "        if USE_CENTER_FALLBACK:\n",
    "            return safe_center_crop(img)\n",
    "        return None\n",
    "\n",
    "    best_det = pick_best_detection(results.detections, w, h)\n",
    "    bbox = best_det.location_data.relative_bounding_box\n",
    "\n",
    "    x = int(bbox.xmin * w)\n",
    "    y = int(bbox.ymin * h)\n",
    "    bw = int(bbox.width * w)\n",
    "    bh = int(bbox.height * h)\n",
    "\n",
    "    x = max(0, x)\n",
    "    y = max(0, y)\n",
    "    bw = max(1, bw)\n",
    "    bh = max(1, bh)\n",
    "\n",
    "    margin_x = int(bw * MARGIN_RATIO)\n",
    "    margin_y = int(bh * MARGIN_RATIO)\n",
    "\n",
    "    x1 = max(0, x - margin_x)\n",
    "    y1 = max(0, y - margin_y)\n",
    "    x2 = min(w, x + bw + margin_x)\n",
    "    y2 = min(h, y + bh + margin_y)\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        # fallback kalau bounding box aneh\n",
    "        return safe_center_crop(img)\n",
    "\n",
    "    crop = img[y1:y2, x1:x2]\n",
    "    if crop.size == 0:\n",
    "        return safe_center_crop(img)\n",
    "\n",
    "    face = Image.fromarray(crop)\n",
    "    return ImageOps.fit(face, TARGET_SIZE, Image.BICUBIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e047274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 4 – ArcMarginProduct & FaceNetArcFace\n",
    "# ============================================\n",
    "import math\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    \"\"\"\n",
    "    ArcFace: cos(theta + m) dengan scaling s.\n",
    "    input: (B, in_features) -> embedding\n",
    "    label: (B,) -> class index\n",
    "    output: (B, out_features) -> logits untuk CrossEntropy\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        s: float = 25.0,\n",
    "        m: float = 0.30,\n",
    "        easy_margin: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.easy_margin = easy_margin\n",
    "\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.clamp(cosine.pow(2), 0.0, 1.0))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, label.view(-1, 1), 1.0)\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output\n",
    "\n",
    "\n",
    "class FaceNetArcFace(nn.Module):\n",
    "    def __init__(self, num_classes: int, embedding_dim: int = 512,\n",
    "                 s: float = 25.0, m: float = 0.30):\n",
    "        super().__init__()\n",
    "        self.backbone = InceptionResnetV1(\n",
    "            pretrained='vggface2',\n",
    "            classify=False\n",
    "        )\n",
    "        in_features = 512\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, embedding_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.arc_margin = ArcMarginProduct(\n",
    "            in_features=embedding_dim,\n",
    "            out_features=num_classes,\n",
    "            s=s,\n",
    "            m=m,\n",
    "            easy_margin=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        feat = self.backbone(x)          # (B, 512)\n",
    "        emb = self.embedding(feat)       # (B, 512)\n",
    "        emb = F.normalize(emb, dim=1)\n",
    "\n",
    "        if labels is None:\n",
    "            # inference: pure cosine logits (tanpa margin)\n",
    "            logits = F.linear(\n",
    "                F.normalize(emb),\n",
    "                F.normalize(self.arc_margin.weight)\n",
    "            )\n",
    "        else:\n",
    "            # training: pakai ArcFace margin\n",
    "            logits = self.arc_margin(emb, labels)\n",
    "\n",
    "        return logits, emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3710d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 5 – Load Model & Preprocess\n",
    "# ============================================\n",
    "def load_model():\n",
    "    model = FaceNetArcFace(\n",
    "        num_classes=num_classes,\n",
    "        embedding_dim=512,\n",
    "        s=25.0,\n",
    "        m=0.30,\n",
    "    )\n",
    "\n",
    "    ckpt = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "    if isinstance(ckpt, dict) and \"model\" in ckpt:\n",
    "        state_dict = ckpt[\"model\"]\n",
    "    else:\n",
    "        state_dict = ckpt\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_model()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "def prepare_image(pil_img: Image.Image) -> torch.Tensor:\n",
    "    pil_img = pil_img.convert(\"RGB\")\n",
    "    x = preprocess(pil_img)\n",
    "    x = x.unsqueeze(0)\n",
    "    return x.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 6 – Prediction Function\n",
    "# ============================================\n",
    "def predict(pil_img: Image.Image):\n",
    "    face = detect_and_crop_from_pil(pil_img)\n",
    "    if face is None:\n",
    "        return None, None\n",
    "\n",
    "    x = prepare_image(face)\n",
    "    with torch.no_grad():\n",
    "        logits, emb = model(x)\n",
    "        pred_idx = logits.argmax(1).item()\n",
    "\n",
    "    return pred_idx, face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd36411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 7 – Scan Test Dataset (subdir per class)\n",
    "# ============================================\n",
    "def scan_test_dataset(root_dir: str):\n",
    "    image_paths = []\n",
    "    class_names = []\n",
    "\n",
    "    classes = sorted(os.listdir(root_dir))\n",
    "    classes = [c for c in classes if os.path.isdir(os.path.join(root_dir, c))]\n",
    "\n",
    "    print(\"Found classes in Test:\", len(classes))\n",
    "\n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(root_dir, cls)\n",
    "        for fname in os.listdir(cls_dir):\n",
    "            if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".webp\")):\n",
    "                fpath = os.path.join(cls_dir, fname)\n",
    "                image_paths.append(fpath)\n",
    "                class_names.append(cls)\n",
    "\n",
    "    print(\"Total test images:\", len(image_paths))\n",
    "    return image_paths, class_names\n",
    "\n",
    "\n",
    "test_paths, test_folder_classes = scan_test_dataset(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 8 – Inference + CSV + Metrics\n",
    "# ============================================\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "all_true = []\n",
    "all_pred = []\n",
    "\n",
    "skipped_io = 0\n",
    "skipped_noface = 0\n",
    "\n",
    "for path, true_class in tqdm(\n",
    "    list(zip(test_paths, test_folder_classes)),\n",
    "    total=len(test_paths),\n",
    "    desc=\"Predicting on Test\"\n",
    "):\n",
    "    # 1) Load image (handle file rusak / format aneh)\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "    except Exception as e:\n",
    "        # sama seperti sebelumnya: skip yang tidak bisa dibuka\n",
    "        print(\"Error loading:\", path, \"|\", e)\n",
    "        skipped_io += 1\n",
    "        continue\n",
    "\n",
    "    # 2) Run prediction (pakai pipeline crop + model)\n",
    "    pred_idx, _ = predict(img)\n",
    "\n",
    "    # Kalau wajah tidak terdeteksi, skip dari evaluasi & csv\n",
    "    if pred_idx is None:\n",
    "        skipped_noface += 1\n",
    "        continue\n",
    "\n",
    "    pred_name = idx_to_name.get(pred_idx, \"UNKNOWN_IDX\")\n",
    "\n",
    "    # 3) Simpan ke CSV (filename, label)\n",
    "    results.append({\n",
    "        \"filename\": os.path.basename(path),\n",
    "        \"label\": pred_name,        # string nama kelas\n",
    "    })\n",
    "\n",
    "    # 4) Kumpulkan y_true dan y_pred untuk metric\n",
    "    true_idx = label_map[true_class]   # true label dari nama folder\n",
    "    all_true.append(true_idx)\n",
    "    all_pred.append(pred_idx)\n",
    "\n",
    "# ---- Save CSV ----\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbaded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Hitung Metric ----\n",
    "acc = accuracy_score(all_true, all_pred)\n",
    "cm = confusion_matrix(all_true, all_pred)\n",
    "report = classification_report(\n",
    "    all_true,\n",
    "    all_pred,\n",
    "    target_names=[idx_to_name[i] for i in range(num_classes)],\n",
    ")\n",
    "\n",
    "print(\"Saved CSV to:\", OUTPUT_CSV)\n",
    "print(\"Test Accuracy :\", acc)\n",
    "print(\"Confusion matrix shape:\", cm.shape)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
